import ffmpeg
import whisper
import json 
import subprocess

test_video = "speech.mp4 (360p).mp4"
extracted = "output.wav"
def extract_audio(video_path, output_wav):
    ffmpeg.input(video_path).output(output_wav, acodec='pcm_s16le', ar=44100, ac=2).run()


#General idea for inner working of audio analysis
#Extract the audio from the mp4 file. Note: may need to work on video file type converter
#extract_audio(test_video, extracted)

#Load video transcription model
model = whisper.load_model("base")

#Transcribe whole video 
#result = model.transcribe(extracted, 
#        language="en",         # force language
#        fp16=False,            # use float32 (important on CPU-only)
#        verbose=True,          # print progress
#       task="transcribe",     # or "translate" to English
#    )


#Use third party llm, ollama 2.0, to piece together which video clips/segments to keep
# and which ones to edit out.
with open("transcript.json", "r") as f:
    transcript = f.read()
#print(transcript)
result = subprocess.run(
    ["ollama", "run", "llama2"],
    input="Give me an animal",              # pass the prompt as stdin
    capture_output=True,
    text=True
)

print(result.stdout)
#Piece together the final video. This is likely for vlogging or other similar 
#types of videos 